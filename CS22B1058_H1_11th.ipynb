{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Sigmoid activation function...\n",
      "Epoch 1/10, Activation: Sigmoid\n",
      "Train Loss: 0.6464, Train Acc: 0.8381\n",
      "Val Loss: 0.2606, Val Acc: 0.9275\n",
      "--------------------------------------------------\n",
      "Epoch 2/10, Activation: Sigmoid\n",
      "Train Loss: 0.2208, Train Acc: 0.9369\n",
      "Val Loss: 0.1775, Val Acc: 0.9486\n",
      "--------------------------------------------------\n",
      "Epoch 3/10, Activation: Sigmoid\n",
      "Train Loss: 0.1540, Train Acc: 0.9551\n",
      "Val Loss: 0.1400, Val Acc: 0.9590\n",
      "--------------------------------------------------\n",
      "Epoch 4/10, Activation: Sigmoid\n",
      "Train Loss: 0.1220, Train Acc: 0.9634\n",
      "Val Loss: 0.1200, Val Acc: 0.9642\n",
      "--------------------------------------------------\n",
      "Epoch 5/10, Activation: Sigmoid\n",
      "Train Loss: 0.0995, Train Acc: 0.9703\n",
      "Val Loss: 0.1068, Val Acc: 0.9677\n",
      "--------------------------------------------------\n",
      "Epoch 6/10, Activation: Sigmoid\n",
      "Train Loss: 0.0848, Train Acc: 0.9750\n",
      "Val Loss: 0.1022, Val Acc: 0.9689\n",
      "--------------------------------------------------\n",
      "Epoch 7/10, Activation: Sigmoid\n",
      "Train Loss: 0.0737, Train Acc: 0.9779\n",
      "Val Loss: 0.0933, Val Acc: 0.9712\n",
      "--------------------------------------------------\n",
      "Epoch 8/10, Activation: Sigmoid\n",
      "Train Loss: 0.0666, Train Acc: 0.9799\n",
      "Val Loss: 0.0884, Val Acc: 0.9723\n",
      "--------------------------------------------------\n",
      "Epoch 9/10, Activation: Sigmoid\n",
      "Train Loss: 0.0589, Train Acc: 0.9818\n",
      "Val Loss: 0.0961, Val Acc: 0.9714\n",
      "--------------------------------------------------\n",
      "Epoch 10/10, Activation: Sigmoid\n",
      "Train Loss: 0.0532, Train Acc: 0.9839\n",
      "Val Loss: 0.0869, Val Acc: 0.9741\n",
      "--------------------------------------------------\n",
      "Training with ReLU activation function...\n",
      "Epoch 1/10, Activation: ReLU\n",
      "Train Loss: 0.4023, Train Acc: 0.8794\n",
      "Val Loss: 0.2334, Val Acc: 0.9302\n",
      "--------------------------------------------------\n",
      "Epoch 2/10, Activation: ReLU\n",
      "Train Loss: 0.1952, Train Acc: 0.9419\n",
      "Val Loss: 0.1458, Val Acc: 0.9572\n",
      "--------------------------------------------------\n",
      "Epoch 3/10, Activation: ReLU\n",
      "Train Loss: 0.1390, Train Acc: 0.9577\n",
      "Val Loss: 0.1327, Val Acc: 0.9591\n",
      "--------------------------------------------------\n",
      "Epoch 4/10, Activation: ReLU\n",
      "Train Loss: 0.1115, Train Acc: 0.9652\n",
      "Val Loss: 0.1054, Val Acc: 0.9676\n",
      "--------------------------------------------------\n",
      "Epoch 5/10, Activation: ReLU\n",
      "Train Loss: 0.0946, Train Acc: 0.9709\n",
      "Val Loss: 0.0963, Val Acc: 0.9696\n",
      "--------------------------------------------------\n",
      "Epoch 6/10, Activation: ReLU\n",
      "Train Loss: 0.0832, Train Acc: 0.9739\n",
      "Val Loss: 0.0999, Val Acc: 0.9682\n",
      "--------------------------------------------------\n",
      "Epoch 7/10, Activation: ReLU\n",
      "Train Loss: 0.0739, Train Acc: 0.9759\n",
      "Val Loss: 0.1005, Val Acc: 0.9674\n",
      "--------------------------------------------------\n",
      "Epoch 8/10, Activation: ReLU\n",
      "Train Loss: 0.0650, Train Acc: 0.9790\n",
      "Val Loss: 0.1037, Val Acc: 0.9693\n",
      "--------------------------------------------------\n",
      "Epoch 9/10, Activation: ReLU\n",
      "Train Loss: 0.0595, Train Acc: 0.9807\n",
      "Val Loss: 0.0938, Val Acc: 0.9736\n",
      "--------------------------------------------------\n",
      "Epoch 10/10, Activation: ReLU\n",
      "Train Loss: 0.0529, Train Acc: 0.9837\n",
      "Val Loss: 0.0993, Val Acc: 0.9713\n",
      "--------------------------------------------------\n",
      "Training with Leaky ReLU activation function...\n",
      "Epoch 1/10, Activation: Leaky ReLU\n",
      "Train Loss: 0.3984, Train Acc: 0.8801\n",
      "Val Loss: 0.2219, Val Acc: 0.9362\n",
      "--------------------------------------------------\n",
      "Epoch 2/10, Activation: Leaky ReLU\n",
      "Train Loss: 0.1857, Train Acc: 0.9434\n",
      "Val Loss: 0.1347, Val Acc: 0.9582\n",
      "--------------------------------------------------\n",
      "Epoch 3/10, Activation: Leaky ReLU\n",
      "Train Loss: 0.1321, Train Acc: 0.9591\n",
      "Val Loss: 0.1148, Val Acc: 0.9653\n",
      "--------------------------------------------------\n",
      "Epoch 4/10, Activation: Leaky ReLU\n",
      "Train Loss: 0.1060, Train Acc: 0.9670\n",
      "Val Loss: 0.1097, Val Acc: 0.9662\n",
      "--------------------------------------------------\n",
      "Epoch 5/10, Activation: Leaky ReLU\n",
      "Train Loss: 0.0919, Train Acc: 0.9708\n",
      "Val Loss: 0.1057, Val Acc: 0.9676\n",
      "--------------------------------------------------\n",
      "Epoch 6/10, Activation: Leaky ReLU\n",
      "Train Loss: 0.0797, Train Acc: 0.9749\n",
      "Val Loss: 0.1552, Val Acc: 0.9530\n",
      "--------------------------------------------------\n",
      "Epoch 7/10, Activation: Leaky ReLU\n",
      "Train Loss: 0.0702, Train Acc: 0.9775\n",
      "Val Loss: 0.0996, Val Acc: 0.9702\n",
      "--------------------------------------------------\n",
      "Epoch 8/10, Activation: Leaky ReLU\n",
      "Train Loss: 0.0622, Train Acc: 0.9803\n",
      "Val Loss: 0.0971, Val Acc: 0.9704\n",
      "--------------------------------------------------\n",
      "Epoch 9/10, Activation: Leaky ReLU\n",
      "Train Loss: 0.0582, Train Acc: 0.9805\n",
      "Val Loss: 0.0940, Val Acc: 0.9727\n",
      "--------------------------------------------------\n",
      "Epoch 10/10, Activation: Leaky ReLU\n",
      "Train Loss: 0.0505, Train Acc: 0.9833\n",
      "Val Loss: 0.0896, Val Acc: 0.9721\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Final Comparison ---\n",
      "Activation: Sigmoid | Final Accuracy: 0.9741 | Training Time: 372.89 sec\n",
      "Activation: ReLU | Final Accuracy: 0.9713 | Training Time: 312.83 sec\n",
      "Activation: Leaky ReLU | Final Accuracy: 0.9721 | Training Time: 296.46 sec\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "# Define the 3-layer neural network\n",
    "class ThreeLayerNN(nn.Module):\n",
    "    def __init__(self, activation_fn):\n",
    "        super(ThreeLayerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the input\n",
    "        x = self.activation_fn(self.fc1(x))\n",
    "        x = self.activation_fn(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation on output layer\n",
    "        return x\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define activation functions\n",
    "activation_fns = {\n",
    "    'Sigmoid': nn.Sigmoid(),\n",
    "    'ReLU': nn.ReLU(),\n",
    "    'Leaky ReLU': nn.LeakyReLU(0.01)\n",
    "}\n",
    "\n",
    "# Function to compute accuracy\n",
    "def compute_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    return correct / len(labels)\n",
    "\n",
    "# Train the network\n",
    "def train_model(activation_fn_name, epochs=10):\n",
    "    model = ThreeLayerNN(activation_fns[activation_fn_name])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += compute_accuracy(outputs, labels)\n",
    "\n",
    "        # Average training loss and accuracy\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_acc += compute_accuracy(outputs, labels)\n",
    "\n",
    "        # Average validation loss and accuracy\n",
    "        val_loss /= len(test_loader)\n",
    "        val_acc /= len(test_loader)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Activation: {activation_fn_name}')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        print('-' * 50)\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    return val_acc, training_time\n",
    "\n",
    "# Compare activation functions\n",
    "results = {}\n",
    "for activation_fn_name in activation_fns:\n",
    "    print(f\"Training with {activation_fn_name} activation function...\")\n",
    "    val_acc, train_time = train_model(activation_fn_name)\n",
    "    results[activation_fn_name] = (val_acc, train_time)\n",
    "\n",
    "# Print final comparison results\n",
    "print(\"\\n--- Final Comparison ---\")\n",
    "for activation, (acc, time_taken) in results.items():\n",
    "    print(f\"Activation: {activation} | Final Accuracy: {acc:.4f} | Training Time: {time_taken:.2f} sec\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
